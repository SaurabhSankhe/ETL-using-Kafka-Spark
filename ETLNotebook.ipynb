{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the environment variable for dpark submit command\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing SparkContext \n",
    "sc = SparkContext(appName=\"PythonSparkStreamingKafka_01\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark Streaming Object\n",
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to create Dataframe from a RDD and saving that Dataframe to Parquet Format\n",
    "def readMyRddsFromKafkaStream( readRdd ):\n",
    "    from pyspark.sql import SparkSession\n",
    "  # Put RDD into a Dataframe\n",
    "    spark = SparkSession(sc)\n",
    "    df = spark.read.json( readRdd )\n",
    "    df.registerTempTable( \"temporary_table\" )\n",
    "    df = spark.sql( \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      temporary_table\n",
    "  \"\"\" )\n",
    "    if df.count()>0:\n",
    "        df.show()\n",
    "        df.write.format(\"parquet\").mode(\"append\").save(\"c:/Users/Saurabh/Desktop/My_Data/Parquet_Files/proto.parquet\")\n",
    "        #df.write.parquet(\"c:/Users/Saurabh/Desktop/Cerebri AI/Parquet_Files/proto.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kafkaStream = KafkaUtils.createStream(ssc, 'localhost:9092', 'spark-streaming', {'numtest':1})\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, ['weatherdata'], {\"metadata.broker.list\": 'localhost:9092'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading json data from each Rdd\n",
    "parsed = kafkaStream.map(lambda v: json.loads(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the data from each batch to console\n",
    "parsed.count().map(lambda x:'Entries in this batch: %s' % x).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting each RDD to Dataframe to save it into parquet file\n",
    "parsed.foreachRDD( lambda myRdd: readMyRddsFromKafkaStream( myRdd ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:34:30\n",
      "-------------------------------------------\n",
      "Entries in this batch: 3\n",
      "\n",
      "+---+------------+-------+\n",
      "|aqi|        date|reading|\n",
      "+---+------------+-------+\n",
      "|172|3/1/10 22:00|      1|\n",
      "|173|3/1/10 23:00|      2|\n",
      "|174| 3/2/10 0:00|      3|\n",
      "+---+------------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:35:00\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+-----------+-------+\n",
      "|aqi|       date|reading|\n",
      "+---+-----------+-------+\n",
      "|174|3/2/10 1:00|      4|\n",
      "|178|3/2/10 2:00|      5|\n",
      "|178|3/2/10 3:00|      6|\n",
      "|182|3/2/10 4:00|      7|\n",
      "|180|3/2/10 5:00|      8|\n",
      "|184|3/2/10 6:00|      9|\n",
      "+---+-----------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:35:30\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+------------+-------+\n",
      "|aqi|        date|reading|\n",
      "+---+------------+-------+\n",
      "|187| 3/2/10 7:00|     10|\n",
      "|194| 3/2/10 8:00|     11|\n",
      "|214| 3/2/10 9:00|     12|\n",
      "|219|3/2/10 10:00|     13|\n",
      "|165|3/2/10 11:00|     14|\n",
      "|155|3/2/10 12:00|     15|\n",
      "+---+------------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:36:00\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+------------+-------+\n",
      "|aqi|        date|reading|\n",
      "+---+------------+-------+\n",
      "|105|3/2/10 13:00|     16|\n",
      "|105|3/2/10 14:00|     17|\n",
      "|102|3/2/10 15:00|     18|\n",
      "| 89|3/2/10 16:00|     19|\n",
      "| 82|3/2/10 17:00|     20|\n",
      "| 84|3/2/10 18:00|     21|\n",
      "+---+------------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:36:30\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+------------+-------+\n",
      "|aqi|        date|reading|\n",
      "+---+------------+-------+\n",
      "| 97|3/2/10 19:00|     22|\n",
      "|122|3/2/10 20:00|     23|\n",
      "|182|3/2/10 21:00|     24|\n",
      "|194|3/2/10 22:00|     25|\n",
      "|189|3/2/10 23:00|     26|\n",
      "|179| 3/3/10 0:00|     27|\n",
      "+---+------------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:37:00\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+-----------+-------+\n",
      "|aqi|       date|reading|\n",
      "+---+-----------+-------+\n",
      "|172|3/3/10 1:00|     28|\n",
      "|170|3/3/10 2:00|     29|\n",
      "|208|3/3/10 3:00|     30|\n",
      "|228|3/3/10 4:00|     31|\n",
      "|192|3/3/10 5:00|     32|\n",
      "|180|3/3/10 6:00|     33|\n",
      "+---+-----------+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-18 04:37:30\n",
      "-------------------------------------------\n",
      "Entries in this batch: 6\n",
      "\n",
      "+---+------------+-------+\n",
      "|aqi|        date|reading|\n",
      "+---+------------+-------+\n",
      "|181| 3/3/10 7:00|     34|\n",
      "|185| 3/3/10 8:00|     35|\n",
      "|187| 3/3/10 9:00|     36|\n",
      "|191|3/3/10 10:00|     37|\n",
      "|198|3/3/10 11:00|     38|\n",
      "|215|3/3/10 12:00|     39|\n",
      "+---+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
